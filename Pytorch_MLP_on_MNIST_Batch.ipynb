{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sc22lg/ML-Notebooks/blob/MNIST_Batching/Pytorch_MLP_on_MNIST_Batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP For MNIST"
      ],
      "metadata": {
        "id": "MTobFaa9HP5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %pip install fancy_einsum\n",
        "  %pip install einops\n",
        "  %pip install keras\n",
        "except:\n",
        "  print(\"Package failed to install\")\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import torch as t\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from dataclasses import dataclass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmKIEPBwH4dk",
        "outputId": "9b8ce173-5e13-46f5-f86d-599eabb290c8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fancy_einsum in /usr/local/lib/python3.11/dist-packages (0.0.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.14.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Investigate MNIST dataset:"
      ],
      "metadata": {
        "id": "JwxUe86wJT9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "#printing the shapes of the vectors\n",
        "print('X_train: ' + str(train_X.shape))\n",
        "print('Y_train: ' + str(train_y.shape))\n",
        "print('X_test:  '  + str(test_X.shape))\n",
        "print('Y_test:  '  + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P_VKxA5JYNz",
        "outputId": "fd9094cb-c7eb-4851-d010-1e386c233d92"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (60000, 28, 28)\n",
            "Y_train: (60000,)\n",
            "X_test:  (10000, 28, 28)\n",
            "Y_test:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# expand each element of train_y to a 10-element tensor\n",
        "# with a 1 in the position corresponding to that element's number\n",
        "train_y_tensor = t.nn.functional.one_hot(t.tensor(train_y).to(t.long), num_classes=10).to(t.float)\n",
        "print('original: ' + str(train_y[0]) + '\\n new: ' + str(train_y_tensor[0]))\n",
        "#repeat for test_y\n",
        "test_y_tensor = t.nn.functional.one_hot(t.tensor(test_y).to(t.long), num_classes=10).to(t.float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdfCXScFiZY9",
        "outputId": "9f8c37ac-3932-46e3-b8e9-0c3bdc1d8611"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original: 5\n",
            " new: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Setup:"
      ],
      "metadata": {
        "id": "EHEhHFZ0LZDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  batch:int = 1\n",
        "  d_img:int = 28\n",
        "  n_first_layer:int = 56\n",
        "  n_hidden:int = 128\n",
        "  n_last_layer:int = 56\n",
        "  n_out:int = 10\n",
        "\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "cuFvqYmmLngb"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP Layer:"
      ],
      "metadata": {
        "id": "QDaVnBh2MxyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_In_Layer(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLP_In_Layer, self).__init__()\n",
        "    self.cfg = cfg\n",
        "\n",
        "    self.in_W = nn.Parameter(t.empty(cfg.d_img, cfg.n_first_layer))\n",
        "    nn.init.kaiming_uniform_(self.in_W, nonlinearity='leaky_relu')\n",
        "\n",
        "    self.in_B = nn.Parameter(t.randn(cfg.n_first_layer), requires_grad=True)\n",
        "    nn.init.zeros_(self.in_B) # Initialize biases to zero\n",
        "\n",
        "    self.activation = nn.LeakyReLU()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, d_img, d_img]\n",
        "    first_layer = einsum('batch d_img d_img, d_img n_layer -> batch n_layer', data_in, self.in_W) + self.in_B\n",
        "    #apply Leaky ReLU function\n",
        "    post_ReLu = self.activation(first_layer)\n",
        "    return post_ReLu\n",
        "\n",
        "class MLP_Out_Layer(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLP_Out_Layer, self).__init__()\n",
        "    self.cfg = cfg\n",
        "\n",
        "    self.out_W = nn.Parameter(t.empty(cfg.n_last_layer, cfg.n_out)) # weights\n",
        "    nn.init.kaiming_uniform_(self.out_W, nonlinearity='leaky_relu') # initialise\n",
        "\n",
        "    self.out_B = nn.Parameter(t.randn(cfg.n_out), requires_grad=True)\n",
        "    nn.init.zeros_(self.out_B) # Initialize biases to zero\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, n_layer]\n",
        "    second_layer = einsum('batch n_layer, n_layer n_out -> batch n_out', data_in, self.out_W) + self.out_B\n",
        "    return second_layer\n",
        "\n",
        "class HiddenLayer(nn.Module):\n",
        "  def __init__(self, dim1:int, dim2:int):\n",
        "    super(HiddenLayer, self).__init__()\n",
        "    self.layer_W = nn.Parameter(t.empty(dim1, dim2)) # where dim1 is output of prev layer and dim2 is output size\n",
        "    nn.init.kaiming_uniform_(self.layer_W, nonlinearity='leaky_relu')\n",
        "\n",
        "    self.layer_B = nn.Parameter(t.randn(dim2), requires_grad=True)\n",
        "    nn.init.zeros_(self.layer_B) # Initialize biases to zero\n",
        "\n",
        "    self.activation = nn.LeakyReLU()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, dim1]\n",
        "    hidden_layer = einsum('batch dim1, dim1 dim2 -> batch dim2', data_in, self.layer_W) + self.layer_B\n",
        "    activated = self.activation(hidden_layer)\n",
        "    return activated\n",
        "\n",
        "class Softmax(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Softmax, self).__init__()\n",
        "\n",
        "  def forward(self, MLP_result):\n",
        "    return nn.functional.softmax(MLP_result, dim=1)"
      ],
      "metadata": {
        "id": "goqgkKxoM1AT"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP Body"
      ],
      "metadata": {
        "id": "Mho0LlQrQqyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLP, self).__init__()\n",
        "    self.cfg = cfg\n",
        "    self.in_layer = MLP_In_Layer(cfg)\n",
        "    self.out_layer = MLP_Out_Layer(cfg)\n",
        "    self.hidden_layer_1 = HiddenLayer(cfg.n_first_layer, cfg.n_hidden)\n",
        "    self.hidden_layer_2 = HiddenLayer(cfg.n_hidden, cfg.n_last_layer)\n",
        "    self.Predictor = Softmax()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, d_img, d_img]\n",
        "    result = self.in_layer(data_in)\n",
        "    result = self.hidden_layer_1(result)\n",
        "    result = self.hidden_layer_2(result)\n",
        "    logits = self.out_layer(result)\n",
        "    return self.Predictor.forward(logits)"
      ],
      "metadata": {
        "id": "LaZWYGwoQtKV"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing Forward Propagation"
      ],
      "metadata": {
        "id": "3YYoe1oKRbKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP = MLP(cfg)\n",
        "for i in range(10):\n",
        "  #create tensor of data, set as floats, add extra 'batch' dimension, run through network\n",
        "  prediction = MLP.forward(t.unsqueeze(t.tensor(train_X[i]).float(), 0))\n",
        "  print('Prediction: ' + str(prediction) + 'Actual: ' + str(train_y[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggQOcUXDRguz",
        "outputId": "3cfcb3ef-492e-47a6-bc72-16dc80c7b5e8",
        "collapsed": true
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 5\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 4.8449e-25, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         4.9672e-07, 0.0000e+00, 0.0000e+00, 1.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 0\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         9.9460e-33, 0.0000e+00, 0.0000e+00, 1.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 4\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         9.9464e-38, 0.0000e+00, 0.0000e+00, 1.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         1.0000e+00, 0.0000e+00, 0.0000e+00, 2.9895e-11]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 9\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         1.0731e-38, 0.0000e+00, 0.0000e+00, 1.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 2\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         6.9336e-41, 0.0000e+00, 0.0000e+00, 1.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], grad_fn=<SoftmaxBackward0>)Actual: 3\n",
            "Prediction: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         1.0000e+00, 0.0000e+00, 0.0000e+00, 3.1987e-26]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "fmZ8W_sYnJuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = t.optim.SGD(MLP.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "TO61fwennaAV"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_every = 5000\n",
        "batch_size = 5 # Define batch size\n",
        "\n",
        "# Iterate over training data in batches\n",
        "for i in range(0, len(train_X), batch_size):\n",
        "  # Zero gradients before calculating gradients for the current step\n",
        "  optimiser.zero_grad()\n",
        "\n",
        "  #create tensor of data, set as floats, add extra 'batch' dimension, run through network\n",
        "  # Scale input data to be between 0 and 1\n",
        "  input_data = t.tensor(train_X[i:i+batch_size]).float() / 255.0\n",
        "  target_data = t.tensor(train_y[i:i+batch_size]).long()\n",
        "\n",
        "  prediction = MLP.forward(input_data)\n",
        "\n",
        "  # Use cross_entropy loss with the target class index\n",
        "  loss = nn.functional.cross_entropy(prediction, target_data)\n",
        "\n",
        "  # train network with loss\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "\n",
        "  if i % log_every == 0:\n",
        "    print('Loss: ' + str(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w-WSGkwdnLnb",
        "outputId": "bf63f3b0-4cef-4a63-a5a2-c5578a7b8c7a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(2.2852, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0874, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0895, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7876, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1415, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1531, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8310, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6563, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5263, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6563, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7501, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0579, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing:"
      ],
      "metadata": {
        "id": "Z7q2VpcyrVuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test model with test_X and test_Y data, output correct guess %\n",
        "correct_guess = 0\n",
        "for i in range(len(test_X)):\n",
        "  prediction = MLP.forward(t.unsqueeze(t.tensor(test_X[i]).float(), 0))\n",
        "  if t.argmax(prediction) == test_y[i]:\n",
        "    correct_guess += 1\n",
        "print('Score: ' +  str(correct_guess) + '/' + str(len(test_X)) + ', ' + str(correct_guess/len(test_X)*100) + '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHj4twkQrZUA",
        "outputId": "dd519346-2b04-4bfd-b632-55e003ecc1ed"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 5483/10000, 54.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Network Visualisation"
      ],
      "metadata": {
        "id": "ObOxpAxNZcCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(MLP.in_layer.in_W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfmiopz-ZfaG",
        "outputId": "f46a7bf6-8115-4b4e-8762-3a0a40cf0868"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.2887,  0.2669,  0.3188,  ..., -0.2101, -0.2045, -0.0300],\n",
            "        [-0.0467,  0.3162, -0.0064,  ...,  0.2124,  0.1044,  0.3103],\n",
            "        [ 0.3224, -0.2819,  0.1769,  ..., -0.0696, -0.1212, -0.0387],\n",
            "        ...,\n",
            "        [-0.0972,  0.1693, -0.0114,  ..., -0.1846,  0.1054,  0.0110],\n",
            "        [ 0.3122,  0.1252, -0.2536,  ..., -0.0353,  0.0583,  0.0202],\n",
            "        [ 0.0724, -0.0388,  0.2496,  ..., -0.1366,  0.2779,  0.2713]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    }
  ]
}