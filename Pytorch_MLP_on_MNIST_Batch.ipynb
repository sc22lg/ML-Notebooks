{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sc22lg/ML-Notebooks/blob/MNIST_Batching/Pytorch_MLP_on_MNIST_Batch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP For MNIST"
      ],
      "metadata": {
        "id": "MTobFaa9HP5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %pip install fancy_einsum\n",
        "  %pip install einops\n",
        "  %pip install keras\n",
        "except:\n",
        "  print(\"Package failed to install\")\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import torch as t\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from dataclasses import dataclass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmKIEPBwH4dk",
        "outputId": "2d2d4638-d16a-4e23-d38f-09eb7d8361a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fancy_einsum\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: fancy_einsum\n",
            "Successfully installed fancy_einsum-0.0.3\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.14.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Investigate MNIST dataset:"
      ],
      "metadata": {
        "id": "JwxUe86wJT9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "#printing the shapes of the vectors\n",
        "print('X_train: ' + str(train_X.shape))\n",
        "print('Y_train: ' + str(train_y.shape))\n",
        "print('X_test:  '  + str(test_X.shape))\n",
        "print('Y_test:  '  + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P_VKxA5JYNz",
        "outputId": "b72b5e48-1521-48dd-e455-886204981d50"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "X_train: (60000, 28, 28)\n",
            "Y_train: (60000,)\n",
            "X_test:  (10000, 28, 28)\n",
            "Y_test:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# expand each element of train_y to a 10-element tensor\n",
        "# with a 1 in the position corresponding to that element's number\n",
        "train_y_tensor = t.nn.functional.one_hot(t.tensor(train_y).to(t.long), num_classes=10).to(t.float)\n",
        "print('original: ' + str(train_y[0]) + '\\n new: ' + str(train_y_tensor[0]))\n",
        "#repeat for test_y\n",
        "test_y_tensor = t.nn.functional.one_hot(t.tensor(test_y).to(t.long), num_classes=10).to(t.float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdfCXScFiZY9",
        "outputId": "97a39ca4-4402-4d10-cdd0-b416c4c38bee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original: 5\n",
            " new: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Setup:"
      ],
      "metadata": {
        "id": "EHEhHFZ0LZDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  batch:int = 1\n",
        "  d_img:int = 28\n",
        "  n_first_layer:int = 56\n",
        "  n_hidden:int = 16\n",
        "  n_last_layer:int = 56\n",
        "  n_out:int = 10\n",
        "\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "cuFvqYmmLngb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP Layer:"
      ],
      "metadata": {
        "id": "QDaVnBh2MxyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_In_Layer(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLP_In_Layer, self).__init__()\n",
        "    self.cfg = cfg\n",
        "\n",
        "    self.in_W = nn.Parameter(t.empty(cfg.d_img, cfg.n_first_layer))\n",
        "    nn.init.kaiming_uniform_(self.in_W, nonlinearity='leaky_relu')\n",
        "\n",
        "    self.in_B = nn.Parameter(t.randn(cfg.n_first_layer), requires_grad=True)\n",
        "    nn.init.zeros_(self.in_B) # Initialize biases to zero\n",
        "\n",
        "    self.activation = nn.LeakyReLU()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, d_img, d_img]\n",
        "    first_layer = einsum('batch d_img d_img, d_img n_layer -> batch n_layer', data_in, self.in_W) + self.in_B\n",
        "    #apply Leaky ReLU function\n",
        "    post_ReLu = self.activation(first_layer)\n",
        "    return post_ReLu\n",
        "\n",
        "class MLP_Out_Layer(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLP_Out_Layer, self).__init__()\n",
        "    self.cfg = cfg\n",
        "\n",
        "    self.out_W = nn.Parameter(t.empty(cfg.n_last_layer, cfg.n_out)) # weights\n",
        "    nn.init.kaiming_uniform_(self.out_W, nonlinearity='leaky_relu') # initialise\n",
        "\n",
        "    self.out_B = nn.Parameter(t.randn(cfg.n_out), requires_grad=True)\n",
        "    nn.init.zeros_(self.out_B) # Initialize biases to zero\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, n_layer]\n",
        "    second_layer = einsum('batch n_layer, n_layer n_out -> batch n_out', data_in, self.out_W) + self.out_B\n",
        "    return second_layer\n",
        "\n",
        "class HiddenLayer(nn.Module):\n",
        "  def __init__(self, dim1:int, dim2:int):\n",
        "    super(HiddenLayer, self).__init__()\n",
        "    self.layer_W = nn.Parameter(t.empty(dim1, dim2)) # where dim1 is output of prev layer and dim2 is output size\n",
        "    nn.init.kaiming_uniform_(self.layer_W, nonlinearity='leaky_relu')\n",
        "\n",
        "    self.layer_B = nn.Parameter(t.randn(dim2), requires_grad=True)\n",
        "    nn.init.zeros_(self.layer_B) # Initialize biases to zero\n",
        "\n",
        "    self.activation = nn.LeakyReLU()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, dim1]\n",
        "    hidden_layer = einsum('batch dim1, dim1 dim2 -> batch dim2', data_in, self.layer_W) + self.layer_B\n",
        "    activated = self.activation(hidden_layer)\n",
        "    return activated\n",
        "\n",
        "class Softmax(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Softmax, self).__init__()\n",
        "\n",
        "  def forward(self, MLP_result):\n",
        "    return nn.functional.softmax(MLP_result, dim=1)"
      ],
      "metadata": {
        "id": "goqgkKxoM1AT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP Body"
      ],
      "metadata": {
        "id": "Mho0LlQrQqyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLP, self).__init__()\n",
        "    self.cfg = cfg\n",
        "    self.in_layer = MLP_In_Layer(cfg)\n",
        "    self.out_layer = MLP_Out_Layer(cfg)\n",
        "    self.hidden_layer_1 = HiddenLayer(cfg.n_first_layer, cfg.n_hidden)\n",
        "    self.hidden_layer_2 = HiddenLayer(cfg.n_hidden, cfg.n_last_layer)\n",
        "    self.Predictor = Softmax()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, d_img, d_img]\n",
        "    result = self.in_layer(data_in)\n",
        "    result = self.hidden_layer_1(result)\n",
        "    result = self.hidden_layer_2(result)\n",
        "    logits = self.out_layer(result)\n",
        "    return self.Predictor.forward(logits)"
      ],
      "metadata": {
        "id": "LaZWYGwoQtKV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing Forward Propagation"
      ],
      "metadata": {
        "id": "3YYoe1oKRbKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP = MLP(cfg)\n",
        "for i in range(10):\n",
        "  #create tensor of data, set as floats, add extra 'batch' dimension, run through network\n",
        "  prediction = MLP.forward(t.unsqueeze(t.tensor(train_X[i]).float(), 0))\n",
        "  print('Prediction: ' + str(prediction) + 'Actual: ' + str(train_y[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggQOcUXDRguz",
        "outputId": "08f5e2a4-fad7-4188-bbe6-9ba35b80c96c",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: tensor([[0.0000e+00, 4.9830e-24, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
            "         1.1761e-06, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 5\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.2319e-31, 1.0000e+00,\n",
            "         2.2747e-23, 1.8976e-18, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 0\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.4472e-40, 5.6125e-41,\n",
            "         0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 4\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 6.3590e-02, 0.0000e+00, 3.2646e-32,\n",
            "         9.3637e-01, 4.2575e-05, 0.0000e+00, 4.9786e-32]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3631e-44, 0.0000e+00, 0.0000e+00,\n",
            "         1.0000e+00, 1.2412e-13, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 9\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.9310e-04, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 9.9981e-01, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 2\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 3.4612e-43, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 8.4078e-45, 1.0905e-06, 0.0000e+00, 3.4135e-18,\n",
            "         3.3310e-08, 1.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 3\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         8.2758e-10, 5.2669e-15, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "fmZ8W_sYnJuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = t.optim.SGD(MLP.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "TO61fwennaAV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_every = 100\n",
        "# MLP = MLP(cfg) # Removed redundant instantiation\n",
        "for i in range(len(train_X)): # Iterate over training data\n",
        "  # Zero gradients before calculating gradients for the current step\n",
        "  optimiser.zero_grad()\n",
        "\n",
        "  #create tensor of data, set as floats, add extra 'batch' dimension, run through network\n",
        "  # Scale input data to be between 0 and 1\n",
        "  input_data = t.unsqueeze(t.tensor(train_X[i]).float() / 255.0, 0)\n",
        "  prediction = MLP.forward(input_data)\n",
        "\n",
        "  # Use cross_entropy loss with the target class index\n",
        "  loss = nn.functional.cross_entropy(prediction, t.tensor([train_y[i]]).long()) # Target should be class index, not one-hot\n",
        "\n",
        "  # train network with loss\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "\n",
        "  if i % log_every == 0:\n",
        "    print('Loss: ' + str(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w-WSGkwdnLnb",
        "outputId": "eb059152-c41c-4730-e966-2a83d0518f49"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(2.2235, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2303, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2895, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3198, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3655, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2259, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2807, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1785, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2782, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2552, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2829, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2746, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2899, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2473, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3203, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3210, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2095, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2257, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3574, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3414, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3397, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9913, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4262, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5660, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4001, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6729, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4072, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5092, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2997, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3541, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4135, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3280, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1733, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2701, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4307, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4179, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4233, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3384, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4655, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4871, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7111, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3902, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4258, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4661, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4515, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4809, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4448, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7188, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4901, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2709, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4667, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3273, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1342, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4883, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4657, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2936, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4539, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4881, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4715, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4602, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4353, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4549, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5569, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4263, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5528, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4828, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4631, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4706, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4491, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4269, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7498, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4587, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4238, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5182, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4429, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4181, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4358, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4592, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4627, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6207, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4588, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4419, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4573, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3773, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4004, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4535, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4527, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6201, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4607, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4802, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4511, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4166, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4165, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4446, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4573, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4725, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0349, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1482, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4200, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4780, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4528, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4634, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4589, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4581, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4630, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4729, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4630, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4621, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4606, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4493, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4703, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4682, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2912, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4534, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4422, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4483, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4708, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4850, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4458, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3059, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3912, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5237, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6286, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7980, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4499, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4607, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3882, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4637, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4780, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2718, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2398, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4546, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4393, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9785, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4316, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4356, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4418, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0716, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4619, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4603, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4503, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8752, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4599, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4387, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6342, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4572, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4534, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4539, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3599, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4278, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4459, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4629, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4345, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4370, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4586, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5146, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4133, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6510, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4833, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4054, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4218, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4623, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5980, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4541, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4390, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4621, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4561, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2236, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3223, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6600, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4850, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4653, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4464, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4484, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3187, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4385, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3120, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0212, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7741, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4582, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4543, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4714, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8461, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4602, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4672, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4665, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4401, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4766, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6492, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4618, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4713, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4606, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4625, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4240, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4793, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4599, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9832, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4681, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4333, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4605, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5883, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4379, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7307, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4707, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3430, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4409, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4379, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4600, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4633, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4600, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4641, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4623, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4604, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4551, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4618, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2298, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4499, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0269, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9840, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4552, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4339, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4304, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4293, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4635, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3658, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0416, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2241, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4653, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3997, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4667, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4095, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4686, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4607, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4601, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4659, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4583, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4151, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4495, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3566, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4602, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5395, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5465, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4502, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4398, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4604, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6377, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5527, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4759, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4598, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4198, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4373, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8470, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4279, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4608, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4474, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3289, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4605, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6308, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5235, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4683, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4434, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4619, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4269, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4618, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4601, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4250, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4625, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3846, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6153, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8766, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4160, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4593, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2294, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4682, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4591, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4568, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4725, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4277, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4444, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0279, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4800, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5465, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4294, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4601, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4582, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4575, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4511, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6070, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4124, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4616, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8945, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4663, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1504, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4564, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4578, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4514, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4592, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4387, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4621, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5432, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4557, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4272, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4693, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4616, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4514, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4485, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4277, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6423, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4616, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4652, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8222, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4588, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4623, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4403, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4520, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5996, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4632, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4576, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4570, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4241, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4591, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4608, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4624, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6639, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4682, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4705, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4591, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4577, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4705, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4599, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4571, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4654, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4649, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4298, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5600, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4541, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4324, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4569, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3605, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3661, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4479, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4682, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4423, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4624, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2993, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3981, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5190, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4586, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4617, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4626, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4490, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4605, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4832, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4521, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4605, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4252, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9086, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4591, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4062, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4746, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4567, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4147, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4597, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4646, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4620, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4625, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4645, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4616, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4586, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6526, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4605, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4349, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2845, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4241, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3978, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4616, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4526, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4954, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4585, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4647, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4655, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4606, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4917, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4623, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4644, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4617, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4155, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5834, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3029, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5053, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4578, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4830, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7993, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4759, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3040, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4298, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4244, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4549, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4565, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4656, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8494, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4384, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4787, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing:"
      ],
      "metadata": {
        "id": "Z7q2VpcyrVuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test model with test_X and test_Y data, output correct guess %\n",
        "correct_guess = 0\n",
        "for i in range(len(test_X)):\n",
        "  prediction = MLP.forward(t.unsqueeze(t.tensor(test_X[i]).float(), 0))\n",
        "  if t.argmax(prediction) == test_y[i]:\n",
        "    correct_guess += 1\n",
        "print('Score: ' +  str(correct_guess) + '/' + str(len(test_X)) + ', ' + str(correct_guess/len(test_X)*100) + '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHj4twkQrZUA",
        "outputId": "a2a19a2c-40e9-4431-f658-66e7094bcbe9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 4236/10000, 42.36%\n"
          ]
        }
      ]
    }
  ]
}