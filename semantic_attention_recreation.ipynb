{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlqsK3WN+VIo6HToZyFbTe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sc22lg/ML-Notebooks/blob/gpt2-small-paper-recreation/semantic_attention_recreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Recreation of the Results of: The Self-Hating Attention Head: A Deep Dive in GPT-2 - Matteo Migliarini July 2025\n",
        "by Leo Gott\n",
        "\n",
        "Original publication can be found [here](https://www.lesswrong.com/posts/wxPvdBwWeaneAsWRB/the-self-hating-attention-head-a-deep-dive-in-gpt-2-1)"
      ],
      "metadata": {
        "id": "GvOoVJ01k5Ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall idea:\n",
        "\"gpt2-small's head L1H5 directs attention to semantically similar tokens and actively suppresses self-attention\"\n",
        "### Results to re-create:\n",
        "- Create inputs to ellicit expected behaviour\n",
        "- Use inputs to identify heads performing behaviour in gpt2-small (expected head L1H5)\n",
        "- Perform mean-ablation of preceding components to find which components effect L1H5"
      ],
      "metadata": {
        "id": "BkQK5fgbl0_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup:"
      ],
      "metadata": {
        "id": "tDwZTA1Tr4F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import pkg_resources\n",
        "\n",
        "installed_packages = [pkg.key for pkg in pkg_resources.working_set]\n",
        "if \"transformer-lens\" not in installed_packages:\n",
        "    %pip install transformer_lens==2.11.0 einops eindex-callum jaxtyping git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "\n",
        "import pandas as pd\n",
        "import circuitsvis as cv\n",
        "import einops\n",
        "import numpy as np\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from transformer_lens import (\n",
        "    ActivationCache,\n",
        "    FactoredMatrix,\n",
        "    HookedTransformer,\n",
        "    HookedTransformerConfig,\n",
        "    utils,\n",
        ")\n",
        "from transformer_lens.hook_points import HookPoint"
      ],
      "metadata": {
        "id": "oUuoGz1ur6Ct"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rand\n",
        "import plotly.express as px\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "ZDpjg9KCMx3P"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Generate input prompt"
      ],
      "metadata": {
        "id": "a63A0WEToMzV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNsmbpepkrtX",
        "outputId": "0469cd7a-5a7d-4958-ad41-ec6dc6c54389"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0           1          2          3          4          5\n",
            "0   Monday     Tuesday  Wednesday   Thursday     Friday   Saturday\n",
            "1      red        blue      green     silver      white       Blue\n",
            "2     1918        1920       1930       1943       1998       2000\n",
            "3      You          He        his        she        her      their\n",
            "4    Italy     Iceland    Austria     Mexico      Spain     France\n",
            "5      dog         cat      horse       bird       fish     lizard\n",
            "6       60          65         69         70         71         90\n",
            "7    angry       happy        sad    excited      bored   stressed\n",
            "8      car         bus        van      truck  motorbike  aeroplane\n",
            "9     rose       tulip       lily      daisy     orchid  sunflower\n",
            "10  guitar       piano     violin       drum      flute    trumpet\n",
            "11  soccer  basketball     tennis   baseball      rugby     hockey\n",
            "12  circle      square   triangle  rectangle    hexagon    octagon\n",
            "13   chair       table       sofa       desk        bed      shelf\n",
            "14   river        lake      ocean        sea       pond     stream\n"
          ]
        }
      ],
      "source": [
        "semantic_words_file = pd.read_csv('semantic_words.csv', header=None)\n",
        "print(semantic_words_file.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create shuffled list of tokens\n",
        "n_sequences = 30\n",
        "n_tokens = 12\n",
        "n_rows = semantic_words_file.shape[0]\n",
        "\n",
        "inputs = np.empty((n_sequences, n_tokens), dtype=tuple)\n",
        "\n",
        "for i in range(n_sequences):\n",
        "  subset = semantic_words_file.sample(4)\n",
        "  for j in range(n_tokens):\n",
        "    category_list = subset.sample(1)\n",
        "    category = category_list.index[0]\n",
        "    token = category_list.iloc[0].sample(1).values[0]\n",
        "    inputs[i, j] = (category, token)\n",
        "# print(inputs)"
      ],
      "metadata": {
        "id": "kPTOFv_Hu2IW",
        "collapsed": true
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create masks representing where tokens in an input share a category\n",
        "masks = np.zeros((n_sequences, n_tokens, n_tokens))\n",
        "\n",
        "for seq in range(n_sequences):\n",
        "  for i in range(n_tokens):\n",
        "    for j in range(n_tokens):\n",
        "      if inputs[seq, i][0] == inputs[seq, j][0] and inputs[seq, i][1] != inputs[seq, j][1] and i > j: # ensures upper triangle is 0s\n",
        "        masks[seq, i, j] = 1"
      ],
      "metadata": {
        "id": "6zq_q0WrXRfg"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_mask = 1\n",
        "fig = px.imshow(masks[show_mask], labels=dict(x=\"Token Index\", y=\"Token Index\", color=\"Same Category\")) # Added labels dictionary\n",
        "fig.update_layout(xaxis = dict(\n",
        "        tickmode = 'array',\n",
        "        tickvals = list(range(n_tokens)),\n",
        "        ticktext = [inputs[show_mask, i][1] for i in range(n_tokens)] # Use tokens from inputs for x-axis\n",
        "    ),\n",
        "    yaxis = dict(\n",
        "        tickmode = 'array',\n",
        "        tickvals = list(range(n_tokens)),\n",
        "        ticktext = [inputs[show_mask, i][1] for i in range(n_tokens)] # Use tokens from inputs for y-axis\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "2qn3t2CVass-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & test gpt2-small:"
      ],
      "metadata": {
        "id": "QvvWExRYkTdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=\"cpu\")"
      ],
      "metadata": {
        "id": "TlwpExqFkZ9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run model on selected sequence & cache activation\n",
        "sequence_index = 1\n",
        "test_input = ' '.join([inputs[sequence_index, i][1] for i in range(n_tokens)])\n",
        "print(\"input: \" + test_input)\n",
        "input_tokens = model.to_tokens(test_input)\n",
        "logits, cache = model.run_with_cache(input_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yub7MsEKlDa4",
        "outputId": "fa7bd9a3-55a9-418c-a940-f09cae67fcd9"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: blue triangle van Italy Italy Italy green truck circle red square green\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer1_patterns = cache[\"pattern\", 1]\n",
        "print(layer1_patterns.shape)\n",
        "print(input_tokens.shape)\n",
        "print(input_tokens.squeeze())\n",
        "str_tokens = model.to_str_tokens(input_tokens.squeeze())\n",
        "\n",
        "display(\n",
        "    cv.attention.attention_patterns(\n",
        "        tokens=str_tokens,\n",
        "        attention=layer1_patterns.squeeze(),\n",
        "        attention_head_names=[f\"L1H{i}\" for i in range(12)],\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "WtbV05XCm252"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pattern detector:"
      ],
      "metadata": {
        "id": "HW1HJh0ITDjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"layers: {model.cfg.n_layers}\")\n",
        "print(f\"heads per layer: {model.cfg.n_heads}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmgi9TYtTesU",
        "outputId": "30cab146-5a9e-485c-b671-953c1e22c646"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers: 12\n",
            "heads per layer: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_head_detector(cache: ActivationCache):\n",
        "  scores = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
        "\n",
        "  # calculate attention score for current input for each attention head\n",
        "  for layer in range(model.cfg.n_layers):\n",
        "    for head in range(model.cfg.n_heads):\n",
        "      attention_pattern = cache[\"pattern\", layer].squeeze()[head]\n",
        "      expected_attention = attention_pattern[1:,1:][t.from_numpy(masks[sequence_index]).bool()] # slice attention to ignore |endoftext| and then apply mask\n",
        "      scores[layer, head] = t.mean(expected_attention)\n",
        "  return scores\n"
      ],
      "metadata": {
        "id": "iIuPQ2N2TQPE"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_head_scores = semantic_head_detector(cache)\n",
        "fig = px.imshow(attn_head_scores, labels=dict(x=\"Head\", y=\"Layer\", color=\"Attention Score\"))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "zWL-cDBZZYaJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}