{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMDWXCuE6VBaeuz9vCuKQ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sc22lg/ML-Notebooks/blob/gpt2-small-paper-recreation/semantic_attention_recreation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Recreation of the Results of: The Self-Hating Attention Head: A Deep Dive in GPT-2 - Matteo Migliarini July 2025\n",
        "by Leo Gott\n",
        "\n",
        "Original publication can be found [here](https://www.lesswrong.com/posts/wxPvdBwWeaneAsWRB/the-self-hating-attention-head-a-deep-dive-in-gpt-2-1)"
      ],
      "metadata": {
        "id": "GvOoVJ01k5Ru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall idea:\n",
        "\"gpt2-small's head L1H5 directs attention to semantically similar tokens and actively suppresses self-attention\"\n",
        "### Results to re-create:\n",
        "- Create inputs to ellicit expected behaviour\n",
        "- Use inputs to identify heads performing behaviour in gpt2-small (expected head L1H5)\n",
        "- Perform mean-ablation of preceding components to find which components effect L1H5"
      ],
      "metadata": {
        "id": "BkQK5fgbl0_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup:"
      ],
      "metadata": {
        "id": "tDwZTA1Tr4F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import pkg_resources\n",
        "\n",
        "installed_packages = [pkg.key for pkg in pkg_resources.working_set]\n",
        "if \"transformer-lens\" not in installed_packages:\n",
        "    %pip install transformer_lens==2.11.0 einops eindex-callum jaxtyping git+https://github.com/callummcdougall/CircuitsVis.git#subdirectory=python\n",
        "\n",
        "import pandas as pd\n",
        "import circuitsvis as cv\n",
        "import einops\n",
        "import numpy as np\n",
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from transformer_lens import (\n",
        "    ActivationCache,\n",
        "    FactoredMatrix,\n",
        "    HookedTransformer,\n",
        "    HookedTransformerConfig,\n",
        "    utils,\n",
        ")\n",
        "from transformer_lens.hook_points import HookPoint"
      ],
      "metadata": {
        "id": "oUuoGz1ur6Ct"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random as rand\n",
        "import plotly.express as px\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "ZDpjg9KCMx3P"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Generate input prompt"
      ],
      "metadata": {
        "id": "a63A0WEToMzV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNsmbpepkrtX",
        "outputId": "16e5b2cf-163b-4719-cc12-1b874b35f1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        0        1          2         3       4         5\n",
            "0     dog      cat       fish       rat     bat     horse\n",
            "1     red     blue      green      pink    gray      cyan\n",
            "2     you       he        his       she     her     their\n",
            "3     car      bus        van      taxi    jeep      tram\n",
            "4   Italy  Iceland    Austria    Mexico   Spain    France\n",
            "5     sad      mad       glad     angry    calm     happy\n",
            "6  Monday  Tuesday  Wednesday  Thursday  Friday  Saturday\n",
            "7      69       72         88        68      90        57\n",
            "8    1800     2025       1275      1945    1600      2100\n"
          ]
        }
      ],
      "source": [
        "semantic_words_file = pd.read_csv('semantic_words.csv', header=None)\n",
        "print(semantic_words_file.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create shuffled list of tokens\n",
        "n_sequences = 30\n",
        "n_tokens = 12\n",
        "n_rows = semantic_words_file.shape[0]\n",
        "\n",
        "inputs = np.empty((n_sequences, n_tokens), dtype=tuple)\n",
        "\n",
        "for i in range(n_sequences):\n",
        "  subset = semantic_words_file.sample(4)\n",
        "  for j in range(n_tokens):\n",
        "    category_list = subset.sample(1)\n",
        "    category = category_list.index[0]\n",
        "    token = category_list.iloc[0].sample(1).values[0]\n",
        "    inputs[i, j] = (category, token)\n",
        "# print(inputs)"
      ],
      "metadata": {
        "id": "kPTOFv_Hu2IW",
        "collapsed": true
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create masks representing where tokens in an input share a category\n",
        "masks = np.zeros((n_sequences, n_tokens, n_tokens))\n",
        "\n",
        "for seq in range(n_sequences):\n",
        "  for i in range(n_tokens):\n",
        "    for j in range(n_tokens):\n",
        "      if inputs[seq, i][0] == inputs[seq, j][0] and inputs[seq, i][1] != inputs[seq, j][1] and i > j: # ensures upper triangle is 0s\n",
        "        masks[seq, i, j] = 1"
      ],
      "metadata": {
        "id": "6zq_q0WrXRfg"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_mask = 1\n",
        "fig = px.imshow(masks[show_mask], labels=dict(x=\"Token Index\", y=\"Token Index\", color=\"Same Category\")) # Added labels dictionary\n",
        "fig.update_layout(xaxis = dict(\n",
        "        tickmode = 'array',\n",
        "        tickvals = list(range(n_tokens)),\n",
        "        ticktext = [inputs[show_mask, i][1] for i in range(n_tokens)] # Use tokens from inputs for x-axis\n",
        "    ),\n",
        "    yaxis = dict(\n",
        "        tickmode = 'array',\n",
        "        tickvals = list(range(n_tokens)),\n",
        "        ticktext = [inputs[show_mask, i][1] for i in range(n_tokens)] # Use tokens from inputs for y-axis\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "2qn3t2CVass-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load & test gpt2-small:"
      ],
      "metadata": {
        "id": "QvvWExRYkTdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=\"cpu\")"
      ],
      "metadata": {
        "id": "TlwpExqFkZ9w",
        "outputId": "c2163030-b0ba-4d7f-8b10-3b2f98cd8e97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run model on selected sequence & cache activation\n",
        "sequence_index = show_mask\n",
        "test_input = ' '.join([inputs[sequence_index, i][1] for i in range(n_tokens)])\n",
        "print(\"input: \" + test_input)\n",
        "input_tokens = model.to_tokens(test_input)\n",
        "logits, cache = model.run_with_cache(input_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yub7MsEKlDa4",
        "outputId": "8d773346-b405-46b4-c144-7e07dcba569e"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: France 1945 dog cat Iceland France rat Spain angry Italy sad rat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_number = 1\n",
        "layer1_patterns = cache[\"pattern\", layer_number]\n",
        "print(layer1_patterns.shape)\n",
        "print(input_tokens.shape)\n",
        "print(input_tokens.squeeze())\n",
        "str_tokens = model.to_str_tokens(input_tokens.squeeze())\n",
        "\n",
        "display(\n",
        "    cv.attention.attention_patterns(\n",
        "        tokens=str_tokens,\n",
        "        attention=layer1_patterns.squeeze(),\n",
        "        attention_head_names=[f\"L{layer_number}H{i}\" for i in range(12)],\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "WtbV05XCm252"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pattern detector:"
      ],
      "metadata": {
        "id": "HW1HJh0ITDjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"layers: {model.cfg.n_layers}\")\n",
        "print(f\"heads per layer: {model.cfg.n_heads}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmgi9TYtTesU",
        "outputId": "0df6bfd5-5512-43d6-a815-36f4d1e6e198"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers: 12\n",
            "heads per layer: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_semantic_head(cache: ActivationCache, layer:int, head:int):\n",
        "  attention_pattern = cache[\"pattern\", layer].squeeze()[head]\n",
        "  expected_attention = attention_pattern[1:,1:][t.from_numpy(masks[sequence_index]).bool()] # slice attention to ignore |endoftext| and then apply mask\n",
        "  return t.mean(expected_attention)\n",
        "\n",
        "def semantic_head_detector(cache: ActivationCache):\n",
        "  scores = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
        "\n",
        "  # calculate attention score for current input for each attention head\n",
        "  for layer in range(model.cfg.n_layers):\n",
        "    for head in range(model.cfg.n_heads):\n",
        "      scores[layer, head] = evaluate_semantic_head(cache, layer, head)\n",
        "  return scores\n"
      ],
      "metadata": {
        "id": "iIuPQ2N2TQPE"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_head_scores = semantic_head_detector(cache)\n",
        "fig = px.imshow(attn_head_scores, labels=dict(x=\"Head\", y=\"Layer\", color=\"Attention Score\"))\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "zWL-cDBZZYaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measuring Component importance:\n",
        "In order to measure which components are contributing to the function of L1H5, we perform a mean ablation study. We replace each previous compenent of the network with its mean and re-measure the attention score of our metric for L1H5 to see if it was impacted by the ablation of that component. The components which when ablated has the most effect on the performance of L1H5 are the components which contribute to its function."
      ],
      "metadata": {
        "id": "e0RW1Cvj2Jzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#perform mean ablation on positional embedding\n"
      ],
      "metadata": {
        "id": "mOfOpkjC21An"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}