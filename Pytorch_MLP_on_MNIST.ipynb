{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhHd4yvqCLVFfMEyUkAOLU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sc22lg/ML-Notebooks/blob/MLP_in_Pytorch/Pytorch_MLP_on_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP For MNIST"
      ],
      "metadata": {
        "id": "MTobFaa9HP5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %pip install fancy_einsum\n",
        "  %pip install einops\n",
        "  %pip install keras\n",
        "except:\n",
        "  print(\"Package failed to install\")\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import torch as t\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from dataclasses import dataclass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmKIEPBwH4dk",
        "outputId": "7a033390-08eb-48c0-89bf-0550a81b6aeb"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fancy_einsum in /usr/local/lib/python3.11/dist-packages (0.0.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.14.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Investigate MNIST dataset:"
      ],
      "metadata": {
        "id": "JwxUe86wJT9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "#printing the shapes of the vectors\n",
        "print('X_train: ' + str(train_X.shape))\n",
        "print('Y_train: ' + str(train_y.shape))\n",
        "print('X_test:  '  + str(test_X.shape))\n",
        "print('Y_test:  '  + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P_VKxA5JYNz",
        "outputId": "0c2d7f6f-bfa2-4790-8a9d-3311b57bdad1"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (60000, 28, 28)\n",
            "Y_train: (60000,)\n",
            "X_test:  (10000, 28, 28)\n",
            "Y_test:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# expand each element of train_y to a 10-element tensor\n",
        "# with a 1 in the position corresponding to that element's number\n",
        "train_y_tensor = t.nn.functional.one_hot(t.tensor(train_y).to(t.long), num_classes=10).to(t.float)\n",
        "print('original: ' + str(train_y[0]) + '\\n new: ' + str(train_y_tensor[0]))\n",
        "#repeat for test_y\n",
        "test_y_tensor = t.nn.functional.one_hot(t.tensor(test_y).to(t.long), num_classes=10).to(t.float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdfCXScFiZY9",
        "outputId": "c6f28f93-cc4a-4434-de07-d3115f712068"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original: 5\n",
            " new: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Setup:"
      ],
      "metadata": {
        "id": "EHEhHFZ0LZDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  batch:int = 1\n",
        "  d_img:int = 28\n",
        "  n_layer:int = 56\n",
        "  n_out:int = 10\n",
        "\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "cuFvqYmmLngb"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP Layer:"
      ],
      "metadata": {
        "id": "QDaVnBh2MxyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPLayer(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLPLayer, self).__init__()\n",
        "    self.cfg = cfg\n",
        "    self.in_W = nn.Parameter(t.randn(cfg.d_img, cfg.n_layer), requires_grad=True)\n",
        "    self.in_B = nn.Parameter(t.randn(cfg.n_layer), requires_grad=True)\n",
        "    self.out_W = nn.Parameter(t.randn(cfg.n_layer, cfg.n_out), requires_grad=True)\n",
        "    self.out_B = nn.Parameter(t.randn(cfg.n_out), requires_grad=True)\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, d_img, d_img]\n",
        "    first_layer = einsum('batch d_img d_img, d_img n_layer -> batch n_layer', data_in, self.in_W) + self.in_B\n",
        "    #apply ReLU function\n",
        "    post_ReLu = t.maximum(first_layer, t.zeros_like(first_layer))\n",
        "    #second matrix:\n",
        "    second_layer = einsum('batch n_layer, n_layer n_out -> batch n_out', post_ReLu, self.out_W) + self.out_B\n",
        "    return second_layer"
      ],
      "metadata": {
        "id": "goqgkKxoM1AT"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prediction Extraction"
      ],
      "metadata": {
        "id": "bb3z_C68QG3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Softmax, self).__init__()\n",
        "\n",
        "  def forward(self, MLP_result):\n",
        "    return nn.functional.softmax(MLP_result, dim=1)\n"
      ],
      "metadata": {
        "id": "oqJVyTvlQKJ1"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP Body"
      ],
      "metadata": {
        "id": "Mho0LlQrQqyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLP, self).__init__()\n",
        "    self.cfg = cfg\n",
        "    self.layer = MLPLayer(cfg)\n",
        "    self.Predictor = Softmax()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, d_img, d_img]\n",
        "    MLP_result = self.layer(data_in)\n",
        "    return self.Predictor.forward(MLP_result)"
      ],
      "metadata": {
        "id": "LaZWYGwoQtKV"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing Forward Propagation"
      ],
      "metadata": {
        "id": "3YYoe1oKRbKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP = MLP(cfg)\n",
        "for i in range(10):\n",
        "  #create tensor of data, set as floats, add extra 'batch' dimension, run through network\n",
        "  prediction = MLP.forward(t.unsqueeze(t.tensor(train_X[i]).float(), 0))\n",
        "  print('Prediction: ' + str(prediction) + 'Actual: ' + str(train_y[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggQOcUXDRguz",
        "outputId": "8c58d89a-8d1b-4e37-9775-1de74565cc9a",
        "collapsed": true
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 5\n",
            "Prediction: tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 0\n",
            "Prediction: tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 4\n",
            "Prediction: tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 9\n",
            "Prediction: tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 2\n",
            "Prediction: tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 3\n",
            "Prediction: tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "fmZ8W_sYnJuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = t.optim.SGD(MLP.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "TO61fwennaAV"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_every = 100\n",
        "for i in range(len(test_X)):\n",
        "  #create tensor of data, set as floats, add extra 'batch' dimension, run through network\n",
        "  prediction = MLP.forward(t.unsqueeze(t.tensor(train_X[i]).float(), 0))\n",
        "  #find negative log likelyhood loss of prediction and corresponding test_y\n",
        "  loss = nn.functional.binary_cross_entropy(prediction, train_y_tensor[i].unsqueeze(0))\n",
        "  # train network with loss\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "  if i % log_every == 0:\n",
        "    print('Loss: ' + str(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w-WSGkwdnLnb",
        "outputId": "ad3b5079-176d-4234-f0d5-59265942806e"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n",
            "Loss: tensor(20., grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing:"
      ],
      "metadata": {
        "id": "Z7q2VpcyrVuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test model with test_X and test_Y data, output correct guess %\n",
        "correct_guess = 0\n",
        "for i in range(len(test_X)):\n",
        "  prediction = MLP.forward(t.unsqueeze(t.tensor(test_X[i]).float(), 0))\n",
        "  if t.argmax(prediction) == test_y[i]:\n",
        "    correct_guess += 1\n",
        "print('Score: ' +  str(correct_guess) + '/' + str(len(test_X)) + ', ' + str(correct_guess/len(test_X)*100) + '%')"
      ],
      "metadata": {
        "id": "XHj4twkQrZUA",
        "outputId": "f84ba146-08e4-4484-d0d7-0227fff1ca5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 1010/10000, 10.100000000000001%\n"
          ]
        }
      ]
    }
  ]
}