{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZwOQdTe2jMfSfW48bc4WF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sc22lg/ML-Notebooks/blob/MLP_in_Pytorch/Pytorch_MLP_on_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP For MNIST"
      ],
      "metadata": {
        "id": "MTobFaa9HP5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %pip install fancy_einsum\n",
        "  %pip install einops\n",
        "  %pip install keras\n",
        "except:\n",
        "  print(\"Package failed to install\")\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import torch as t\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from dataclasses import dataclass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmKIEPBwH4dk",
        "outputId": "4e421f39-0556-4489-954a-18803977fcf5"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fancy_einsum in /usr/local/lib/python3.11/dist-packages (0.0.3)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.14.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Investigate MNIST dataset:"
      ],
      "metadata": {
        "id": "JwxUe86wJT9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "#printing the shapes of the vectors\n",
        "print('X_train: ' + str(train_X.shape))\n",
        "print('Y_train: ' + str(train_y.shape))\n",
        "print('X_test:  '  + str(test_X.shape))\n",
        "print('Y_test:  '  + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P_VKxA5JYNz",
        "outputId": "09d7b079-e0a6-4b0c-ecc8-2d0198ca7336"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train: (60000, 28, 28)\n",
            "Y_train: (60000,)\n",
            "X_test:  (10000, 28, 28)\n",
            "Y_test:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# expand each element of train_y to a 10-element tensor\n",
        "# with a 1 in the position corresponding to that element's number\n",
        "train_y_tensor = t.nn.functional.one_hot(t.tensor(train_y).to(t.long), num_classes=10).to(t.float)\n",
        "print('original: ' + str(train_y[0]) + '\\n new: ' + str(train_y_tensor[0]))\n",
        "#repeat for test_y\n",
        "test_y_tensor = t.nn.functional.one_hot(t.tensor(test_y).to(t.long), num_classes=10).to(t.float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdfCXScFiZY9",
        "outputId": "24cf4ea5-3fe0-4bbc-db79-f7bbea6d7559"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original: 5\n",
            " new: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Setup:"
      ],
      "metadata": {
        "id": "EHEhHFZ0LZDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  batch:int = 1\n",
        "  d_img:int = 28\n",
        "  n_layer:int = 56\n",
        "  n_out:int = 10\n",
        "\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "cuFvqYmmLngb"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP Layer:"
      ],
      "metadata": {
        "id": "QDaVnBh2MxyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPLayer(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLPLayer, self).__init__()\n",
        "    self.cfg = cfg\n",
        "    # Use Kaiming He initialization for weights\n",
        "    self.in_W = nn.Parameter(t.empty(cfg.d_img, cfg.n_layer))\n",
        "    nn.init.kaiming_uniform_(self.in_W, nonlinearity='leaky_relu')\n",
        "\n",
        "    self.in_B = nn.Parameter(t.randn(cfg.n_layer), requires_grad=True) # Biases often initialized to zero or a small constant\n",
        "    nn.init.zeros_(self.in_B) # Initialize biases to zero\n",
        "\n",
        "    self.out_W = nn.Parameter(t.empty(cfg.n_layer, cfg.n_out))\n",
        "    nn.init.kaiming_uniform_(self.out_W, nonlinearity='leaky_relu')\n",
        "\n",
        "    self.out_B = nn.Parameter(t.randn(cfg.n_out), requires_grad=True) # Biases often initialized to zero or a small constant\n",
        "    nn.init.zeros_(self.out_B) # Initialize biases to zero\n",
        "\n",
        "    self.activation = nn.LeakyReLU()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, d_img, d_img]\n",
        "    first_layer = einsum('batch d_img d_img, d_img n_layer -> batch n_layer', data_in, self.in_W) + self.in_B\n",
        "    #apply Leaky ReLU function\n",
        "    post_ReLu = self.activation(first_layer)\n",
        "    #second matrix:\n",
        "    second_layer = einsum('batch n_layer, n_layer n_out -> batch n_out', post_ReLu, self.out_W) + self.out_B\n",
        "    return second_layer"
      ],
      "metadata": {
        "id": "goqgkKxoM1AT"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prediction Extraction"
      ],
      "metadata": {
        "id": "bb3z_C68QG3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Softmax, self).__init__()\n",
        "\n",
        "  def forward(self, MLP_result):\n",
        "    return nn.functional.softmax(MLP_result, dim=1)\n"
      ],
      "metadata": {
        "id": "oqJVyTvlQKJ1"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP Body"
      ],
      "metadata": {
        "id": "Mho0LlQrQqyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLP, self).__init__()\n",
        "    self.cfg = cfg\n",
        "    self.layer = MLPLayer(cfg)\n",
        "    self.Predictor = Softmax()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, d_img, d_img]\n",
        "    MLP_result = self.layer(data_in)\n",
        "    return self.Predictor.forward(MLP_result)"
      ],
      "metadata": {
        "id": "LaZWYGwoQtKV"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing Forward Propagation"
      ],
      "metadata": {
        "id": "3YYoe1oKRbKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP = MLP(cfg)\n",
        "for i in range(10):\n",
        "  #create tensor of data, set as floats, add extra 'batch' dimension, run through network\n",
        "  prediction = MLP.forward(t.unsqueeze(t.tensor(train_X[i]).float(), 0))\n",
        "  print('Prediction: ' + str(prediction) + 'Actual: ' + str(train_y[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggQOcUXDRguz",
        "outputId": "108d00d4-2626-4d22-bdbd-272673e91561",
        "collapsed": true
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 5\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 5.6606e-10]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 0\n",
            "Prediction: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], grad_fn=<SoftmaxBackward0>)Actual: 4\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.8687e-37,\n",
            "         0.0000e+00, 0.0000e+00, 3.4663e-35, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7947e-16, 0.0000e+00, 1.0000e+00,\n",
            "         3.6337e-29, 3.2230e-44, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 9\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 1.1629e-38, 1.7620e-24]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 2\n",
            "Prediction: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 9.8091e-45,\n",
            "         0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 3\n",
            "Prediction: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 1.2205e-04, 3.3037e-40, 9.9988e-01,\n",
            "         0.0000e+00, 0.0000e+00, 5.6063e-20, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], grad_fn=<SoftmaxBackward0>)Actual: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "fmZ8W_sYnJuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = t.optim.SGD(MLP.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "TO61fwennaAV"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_every = 100\n",
        "# MLP = MLP(cfg) # Removed redundant instantiation\n",
        "for i in range(len(train_X)): # Iterate over training data\n",
        "  # Zero gradients before calculating gradients for the current step\n",
        "  optimiser.zero_grad()\n",
        "\n",
        "  #create tensor of data, set as floats, add extra 'batch' dimension, run through network\n",
        "  # Scale input data to be between 0 and 1\n",
        "  input_data = t.unsqueeze(t.tensor(train_X[i]).float() / 255.0, 0)\n",
        "  prediction = MLP.forward(input_data)\n",
        "\n",
        "  # Use cross_entropy loss with the target class index\n",
        "  loss = nn.functional.cross_entropy(prediction, t.tensor([train_y[i]]).long()) # Target should be class index, not one-hot\n",
        "\n",
        "  # train network with loss\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "\n",
        "  if i % log_every == 0:\n",
        "    print('Loss: ' + str(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w-WSGkwdnLnb",
        "outputId": "19aed902-80ee-42f0-ddc7-9b366bf2bd4c"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(2.2774, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1862, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2913, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3061, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3933, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3815, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2832, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3417, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0487, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2596, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3719, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3597, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7622, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8954, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9850, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2756, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2590, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6113, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3219, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3700, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3692, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3921, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8459, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3623, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3539, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3924, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3039, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1083, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3860, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3462, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2132, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4011, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3847, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9822, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6578, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3603, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4176, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5017, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5211, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9943, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3753, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9082, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5121, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4949, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4296, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4157, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3792, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4084, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5428, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8067, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4990, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3488, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4694, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2175, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5095, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4116, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3152, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4518, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9797, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3757, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4267, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4494, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9571, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4240, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6878, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6152, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3905, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3818, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4970, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8550, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4178, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2074, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9260, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8325, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8480, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5574, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3732, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4958, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4111, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4060, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3100, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3705, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4227, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4137, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3810, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3406, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4063, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8013, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4070, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4141, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4597, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5082, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5633, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4932, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9191, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4683, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3343, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3431, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2207, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4662, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7037, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4422, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0177, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4068, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5898, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3494, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3537, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4138, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5252, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5823, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1528, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4838, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4512, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5201, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4325, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4194, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5965, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2391, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1640, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4665, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5239, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9514, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5554, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9797, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8153, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3514, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8395, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4559, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4586, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6473, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9987, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3509, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1039, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4268, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4550, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4773, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5145, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2683, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3898, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4763, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4067, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5332, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2300, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2729, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3946, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3404, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4500, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3512, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4663, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4228, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4113, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4778, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8134, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4452, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4150, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6336, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4950, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4143, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5029, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5050, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4133, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4496, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4806, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7048, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4697, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4042, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4224, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5440, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4625, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8473, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4728, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9437, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4172, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6929, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3934, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2769, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0902, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4085, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4699, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4359, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4464, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4377, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4732, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4745, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4007, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7925, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3743, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4674, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3457, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5548, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4942, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4216, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9161, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4684, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4331, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4307, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2904, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4734, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6157, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4402, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2910, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4769, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6832, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4760, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6468, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4747, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9861, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6796, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4054, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4819, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4801, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4779, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4098, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4941, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4129, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6107, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4031, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1125, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7047, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4225, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1556, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4094, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4978, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5544, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4696, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7813, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4802, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4557, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4532, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0871, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4053, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3123, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4141, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9129, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5954, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2218, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8885, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2234, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4661, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6362, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9519, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5021, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4620, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5018, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5194, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3460, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4592, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4375, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4608, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4777, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4852, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4712, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8147, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4597, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9500, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4294, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4303, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4738, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2505, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6042, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4854, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4284, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3428, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4705, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4253, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3325, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4140, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4578, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4943, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4124, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5694, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4728, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0215, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4327, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1719, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4685, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5197, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9250, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4903, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4731, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4605, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4598, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4108, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4547, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8449, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4696, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4556, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4679, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8536, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4599, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7819, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4190, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3858, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4404, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8787, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5018, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5211, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8187, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5589, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4660, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8095, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4506, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5709, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8195, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4315, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4659, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4587, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4583, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4682, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3385, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9658, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4481, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5126, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4681, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4213, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4734, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4179, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9344, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9034, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0801, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2156, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3996, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5735, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4810, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4754, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2262, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4703, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4273, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4719, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4629, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5258, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4750, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4284, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4667, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4227, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7631, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3176, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0197, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4286, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4533, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5374, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4580, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4469, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4625, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4656, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4389, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6519, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4674, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4636, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3915, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4652, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4648, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4996, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4683, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4238, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3832, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4542, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4924, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4132, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6010, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5343, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4619, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2447, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4234, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4598, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4522, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4629, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4231, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2887, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4662, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0759, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3855, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4366, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4527, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0881, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6667, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4669, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6587, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5369, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4537, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4355, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2035, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0170, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2604, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4153, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5947, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4693, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4746, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4970, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4314, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4103, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4582, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4680, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7626, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4722, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1979, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4688, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0875, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4770, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4631, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4581, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9878, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4762, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4150, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4364, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4723, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5311, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5207, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4644, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4439, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4903, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3955, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4908, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4354, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4640, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4657, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4549, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4601, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8971, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4619, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3590, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6581, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4668, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4321, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4326, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4451, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4379, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4636, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5787, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4704, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4509, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4143, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3484, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2270, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4692, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4403, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5123, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7364, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5600, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5716, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5558, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4739, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2926, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4928, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4720, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4145, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4850, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1732, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4652, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6323, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4557, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5359, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4648, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8638, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5750, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4241, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4605, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5773, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4230, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4412, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1533, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1830, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4907, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4539, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4620, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1793, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4638, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4592, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3350, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4638, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4181, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4619, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5144, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3672, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4209, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3916, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4801, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9671, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4597, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4563, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4680, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4703, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9909, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4354, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4969, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2399, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4435, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4313, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4628, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4891, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4706, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4725, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4682, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4674, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2279, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4630, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4534, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4597, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8568, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9271, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4489, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4243, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4375, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4100, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4682, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4485, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4654, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0083, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4642, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5922, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5290, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4604, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4657, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4712, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4890, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4525, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4084, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4499, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4616, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4418, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4470, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3856, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4556, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4200, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5558, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5005, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4135, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4247, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7192, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4294, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4640, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4248, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5134, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4965, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4636, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4752, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9548, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4737, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9439, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4661, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7107, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4686, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4751, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7886, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4530, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8234, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4691, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4617, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2167, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4250, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4618, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4601, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4595, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4269, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4597, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4672, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4585, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6287, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4235, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4992, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5130, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4711, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3885, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4617, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5182, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4405, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9573, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4378, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4551, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5444, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4655, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5028, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4874, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4607, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6180, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4810, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4728, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4154, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4429, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4673, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7095, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4397, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2446, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4154, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing:"
      ],
      "metadata": {
        "id": "Z7q2VpcyrVuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test model with test_X and test_Y data, output correct guess %\n",
        "correct_guess = 0\n",
        "for i in range(len(test_X)):\n",
        "  prediction = MLP.forward(t.unsqueeze(t.tensor(test_X[i]).float(), 0))\n",
        "  if t.argmax(prediction) == test_y[i]:\n",
        "    correct_guess += 1\n",
        "print('Score: ' +  str(correct_guess) + '/' + str(len(test_X)) + ', ' + str(correct_guess/len(test_X)*100) + '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHj4twkQrZUA",
        "outputId": "8d71bba8-0150-4d88-b399-603f52260f4a"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 5583/10000, 55.83%\n"
          ]
        }
      ]
    }
  ]
}