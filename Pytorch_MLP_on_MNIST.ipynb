{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsiQTTng9zAYzxfH38x9rH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sc22lg/ML-Notebooks/blob/MLP_in_Pytorch/Pytorch_MLP_on_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP For MNIST"
      ],
      "metadata": {
        "id": "MTobFaa9HP5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %pip install fancy_einsum\n",
        "  %pip install einops\n",
        "  %pip install keras\n",
        "except:\n",
        "  print(\"Package failed to install\")\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import torch as t\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from dataclasses import dataclass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmKIEPBwH4dk",
        "outputId": "70788173-fe85-492c-d237-411cfb138980"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fancy_einsum\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: fancy_einsum\n",
            "Successfully installed fancy_einsum-0.0.3\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.8.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras) (4.14.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Investigate MNIST dataset:"
      ],
      "metadata": {
        "id": "JwxUe86wJT9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "\n",
        "#printing the shapes of the vectors\n",
        "print('X_train: ' + str(train_X.shape))\n",
        "print('Y_train: ' + str(train_y.shape))\n",
        "print('X_test:  '  + str(test_X.shape))\n",
        "print('Y_test:  '  + str(test_y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P_VKxA5JYNz",
        "outputId": "3b703d7b-f67e-4084-c252-b2f1cd0229ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "X_train: (60000, 28, 28)\n",
            "Y_train: (60000,)\n",
            "X_test:  (10000, 28, 28)\n",
            "Y_test:  (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# expand each element of train_y to a 10-element tensor\n",
        "# with a 1 in the position corresponding to that element's number\n",
        "train_y_tensor = t.nn.functional.one_hot(t.tensor(train_y).to(t.long), num_classes=10).to(t.float)\n",
        "print('original: ' + str(train_y[0]) + '\\n new: ' + str(train_y_tensor[0]))\n",
        "#repeat for test_y\n",
        "test_y_tensor = t.nn.functional.one_hot(t.tensor(test_y).to(t.long), num_classes=10).to(t.float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdfCXScFiZY9",
        "outputId": "43162a62-adfa-45a8-c1dc-d75500b52f35"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original: 5\n",
            " new: tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Setup:"
      ],
      "metadata": {
        "id": "EHEhHFZ0LZDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "  batch:int = 1\n",
        "  d_img:int = 28\n",
        "  n_first_layer:int = 56\n",
        "  n_hidden:int = 16\n",
        "  n_last_layer:int = 56\n",
        "  n_out:int = 10\n",
        "\n",
        "cfg = Config()"
      ],
      "metadata": {
        "id": "cuFvqYmmLngb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP Layer:"
      ],
      "metadata": {
        "id": "QDaVnBh2MxyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_In_Layer(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLP_In_Layer, self).__init__()\n",
        "    self.cfg = cfg\n",
        "\n",
        "    self.in_W = nn.Parameter(t.empty(cfg.d_img, cfg.n_first_layer))\n",
        "    nn.init.kaiming_uniform_(self.in_W, nonlinearity='leaky_relu')\n",
        "\n",
        "    self.in_B = nn.Parameter(t.randn(cfg.n_first_layer), requires_grad=True)\n",
        "    nn.init.zeros_(self.in_B) # Initialize biases to zero\n",
        "\n",
        "    self.activation = nn.LeakyReLU()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, d_img, d_img]\n",
        "    first_layer = einsum('batch d_img d_img, d_img n_layer -> batch n_layer', data_in, self.in_W) + self.in_B\n",
        "    #apply Leaky ReLU function\n",
        "    post_ReLu = self.activation(first_layer)\n",
        "    return post_ReLu\n",
        "\n",
        "class MLP_Out_Layer(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLP_Out_Layer, self).__init__()\n",
        "    self.cfg = cfg\n",
        "\n",
        "    self.out_W = nn.Parameter(t.empty(cfg.n_last_layer, cfg.n_out)) # weights\n",
        "    nn.init.kaiming_uniform_(self.out_W, nonlinearity='leaky_relu') # initialise\n",
        "\n",
        "    self.out_B = nn.Parameter(t.randn(cfg.n_out), requires_grad=True)\n",
        "    nn.init.zeros_(self.out_B) # Initialize biases to zero\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, n_layer]\n",
        "    second_layer = einsum('batch n_layer, n_layer n_out -> batch n_out', data_in, self.out_W) + self.out_B\n",
        "    return second_layer\n",
        "\n",
        "class HiddenLayer(nn.Module):\n",
        "  def __init__(self, dim1:int, dim2:int):\n",
        "    super(HiddenLayer, self).__init__()\n",
        "    self.layer_W = nn.Parameter(t.empty(dim1, dim2)) # where dim1 is output of prev layer and dim2 is output size\n",
        "    nn.init.kaiming_uniform_(self.layer_W, nonlinearity='leaky_relu')\n",
        "\n",
        "    self.layer_B = nn.Parameter(t.randn(dim2), requires_grad=True)\n",
        "    nn.init.zeros_(self.layer_B) # Initialize biases to zero\n",
        "\n",
        "    self.activation = nn.LeakyReLU()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, dim1]\n",
        "    hidden_layer = einsum('batch dim1, dim1 dim2 -> batch dim2', data_in, self.layer_W) + self.layer_B\n",
        "    activated = self.activation(hidden_layer)\n",
        "    return activated\n",
        "\n",
        "class Softmax(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Softmax, self).__init__()\n",
        "\n",
        "  def forward(self, MLP_result):\n",
        "    return nn.functional.softmax(MLP_result, dim=1)"
      ],
      "metadata": {
        "id": "goqgkKxoM1AT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP Body"
      ],
      "metadata": {
        "id": "Mho0LlQrQqyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super(MLP, self).__init__()\n",
        "    self.cfg = cfg\n",
        "    self.in_layer = MLP_In_Layer(cfg)\n",
        "    self.out_layer = MLP_Out_Layer(cfg)\n",
        "    self.hidden_layer_1 = HiddenLayer(cfg.n_first_layer, cfg.n_hidden)\n",
        "    self.hidden_layer_2 = HiddenLayer(cfg.n_hidden, cfg.n_last_layer)\n",
        "    self.Predictor = Softmax()\n",
        "\n",
        "  def forward(self, data_in):\n",
        "    #data_in format: [batch, d_img, d_img]\n",
        "    result = self.in_layer(data_in)\n",
        "    result = self.hidden_layer_1(result)\n",
        "    result = self.hidden_layer_2(result)\n",
        "    logits = self.out_layer(result)\n",
        "    return self.Predictor.forward(logits)"
      ],
      "metadata": {
        "id": "LaZWYGwoQtKV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing Forward Propagation"
      ],
      "metadata": {
        "id": "3YYoe1oKRbKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP = MLP(cfg)\n",
        "for i in range(10):\n",
        "  #create tensor of data, set as floats, add extra 'batch' dimension, run through network\n",
        "  prediction = MLP.forward(t.unsqueeze(t.tensor(train_X[i]).float(), 0))\n",
        "  print('Prediction: ' + str(prediction) + 'Actual: ' + str(train_y[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggQOcUXDRguz",
        "outputId": "6a3be052-cb05-4e44-e24b-803a725810ce",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: tensor([[1.0000e+00, 2.0930e-23, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 5\n",
            "Prediction: tensor([[2.4962e-32, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.6537e-15,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 0\n",
            "Prediction: tensor([[2.4627e-06, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 4\n",
            "Prediction: tensor([[1.1589e-23, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 2.4750e-37, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[0.0201, 0.9799, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "         0.0000]], grad_fn=<SoftmaxBackward0>)Actual: 9\n",
            "Prediction: tensor([[1.0000e+00, 2.0687e-26, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 2\n",
            "Prediction: tensor([[2.4412e-13, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[1.0000e+00, 4.4736e-19, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 3\n",
            "Prediction: tensor([[3.7808e-06, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 1\n",
            "Prediction: tensor([[3.6025e-06, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.6341e-19]],\n",
            "       grad_fn=<SoftmaxBackward0>)Actual: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "fmZ8W_sYnJuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = t.optim.SGD(MLP.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "TO61fwennaAV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_every = 100\n",
        "# MLP = MLP(cfg) # Removed redundant instantiation\n",
        "for i in range(len(train_X)): # Iterate over training data\n",
        "  # Zero gradients before calculating gradients for the current step\n",
        "  optimiser.zero_grad()\n",
        "\n",
        "  #create tensor of data, set as floats, add extra 'batch' dimension, run through network\n",
        "  # Scale input data to be between 0 and 1\n",
        "  input_data = t.unsqueeze(t.tensor(train_X[i]).float() / 255.0, 0)\n",
        "  prediction = MLP.forward(input_data)\n",
        "\n",
        "  # Use cross_entropy loss with the target class index\n",
        "  loss = nn.functional.cross_entropy(prediction, t.tensor([train_y[i]]).long()) # Target should be class index, not one-hot\n",
        "\n",
        "  # train network with loss\n",
        "  loss.backward()\n",
        "  optimiser.step()\n",
        "\n",
        "  if i % log_every == 0:\n",
        "    print('Loss: ' + str(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w-WSGkwdnLnb",
        "outputId": "02c6a1c7-5991-4912-86a8-f7d88799c0b0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: tensor(2.3303, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3428, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1030, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2818, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3178, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4037, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3502, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3385, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2372, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3102, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7012, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3561, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5353, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2871, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3354, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2657, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6419, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5007, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3419, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3863, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4577, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5162, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3916, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3628, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4080, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5199, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0741, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4157, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4053, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3977, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3268, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4273, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4139, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4294, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4521, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3071, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4473, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4492, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4690, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4734, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9575, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4239, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7971, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4650, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3884, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4658, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4120, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4281, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4058, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4812, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5369, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5010, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3112, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4666, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3977, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4817, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3416, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4315, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4586, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4390, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4339, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3198, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3325, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4799, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5588, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5366, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5294, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3772, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4657, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4481, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4165, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5052, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4601, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4268, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3903, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5636, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4430, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4704, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4219, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3979, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2750, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4538, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4543, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4279, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3999, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4608, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4699, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4555, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4553, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4189, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5456, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4646, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4659, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4405, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4272, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4119, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2974, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4402, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4646, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3933, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4491, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4477, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3829, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7188, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3916, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5123, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4544, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4729, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6138, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8905, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4682, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3852, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4631, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4568, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4156, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4299, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5871, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0729, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4626, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6252, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4768, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5281, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2326, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4668, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7968, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6302, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4296, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4976, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3997, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4030, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9088, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3969, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4582, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6130, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4704, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3709, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2397, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4633, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1297, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4684, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3934, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4480, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4062, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3077, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4497, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4310, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4279, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4344, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4713, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4889, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4463, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4570, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4450, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4696, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1157, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4012, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4568, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4617, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5371, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6404, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4139, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9422, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4617, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4629, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8999, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4628, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0908, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4137, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4855, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4555, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2319, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4834, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2437, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4220, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4368, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4424, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9493, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4627, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4285, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4854, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4245, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4617, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3491, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6137, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4667, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4403, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2927, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4617, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4581, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4401, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4535, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4974, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4379, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4242, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3187, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4723, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5471, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5407, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4271, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4655, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5444, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6246, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4598, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4641, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2539, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0116, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4602, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8068, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2714, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4555, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4639, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4595, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6061, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4921, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4661, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1769, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4698, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4621, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4605, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4271, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4083, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4565, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4597, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4881, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8933, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1725, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4757, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4338, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4638, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4836, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4690, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4646, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6995, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4539, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4773, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4652, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4802, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4606, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4605, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7433, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2825, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5027, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4255, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4046, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6195, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4233, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4356, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4216, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4627, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4146, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4661, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4648, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4790, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4245, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4068, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4650, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4864, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5283, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4580, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4608, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9488, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4542, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4629, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4624, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4596, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3930, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4273, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4311, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6058, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0523, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5098, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5107, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4630, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.7784, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4854, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4810, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4863, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4532, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4604, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4249, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2526, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4608, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4626, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4621, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4243, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4497, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4589, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6153, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4533, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9136, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3606, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4851, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4676, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4747, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2462, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4580, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4620, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4617, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4442, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4609, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0725, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5965, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4957, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3943, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4520, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4253, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4808, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4832, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3874, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4620, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5344, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4118, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4546, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4621, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4377, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4691, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5422, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4624, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4591, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4563, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4653, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4546, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4474, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4554, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4456, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4903, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5106, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4627, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4623, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4606, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4562, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4296, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4553, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4847, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4623, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5292, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0753, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4390, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5138, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4592, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9295, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4541, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4213, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4619, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4578, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1830, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5087, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4681, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9187, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4527, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4657, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4444, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6472, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4266, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4158, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4594, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2427, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3830, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4175, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3005, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4699, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4603, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6253, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4688, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4653, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6930, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5587, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9045, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5072, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4369, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4662, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4594, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5101, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4632, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2725, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3806, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.2821, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4587, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0859, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4640, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.5006, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1952, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4190, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4618, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4557, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4211, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4149, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4603, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4608, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4858, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4146, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4592, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4615, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6104, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4581, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4580, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4616, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4616, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4712, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4559, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9544, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4119, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4515, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4098, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4657, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4713, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.6923, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4670, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4546, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.8722, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4610, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4507, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4430, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4608, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.0337, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4649, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4620, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4292, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4242, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4616, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4654, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4618, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4632, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4623, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4206, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4621, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.9433, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4614, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4654, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1748, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4606, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4397, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4622, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4244, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4600, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4515, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4516, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4629, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4613, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4618, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.3429, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4607, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.1292, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4799, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4236, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4611, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4612, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(1.4618, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4537, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4150, grad_fn=<NllLossBackward0>)\n",
            "Loss: tensor(2.4219, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing:"
      ],
      "metadata": {
        "id": "Z7q2VpcyrVuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test model with test_X and test_Y data, output correct guess %\n",
        "correct_guess = 0\n",
        "for i in range(len(test_X)):\n",
        "  prediction = MLP.forward(t.unsqueeze(t.tensor(test_X[i]).float(), 0))\n",
        "  if t.argmax(prediction) == test_y[i]:\n",
        "    correct_guess += 1\n",
        "print('Score: ' +  str(correct_guess) + '/' + str(len(test_X)) + ', ' + str(correct_guess/len(test_X)*100) + '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHj4twkQrZUA",
        "outputId": "1f76b85d-4533-47ee-e94a-9b06e6632b0a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 5326/10000, 53.26%\n"
          ]
        }
      ]
    }
  ]
}